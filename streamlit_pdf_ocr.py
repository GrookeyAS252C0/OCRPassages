import streamlit as st
import json
import tempfile
import os
from pathlib import Path
from datetime import datetime
import io
import zipfile

# Streamlit Cloudç’°å¢ƒã§ã®NLTKãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
try:
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
except:
    pass

from pdf_text_extractor import PDFTextExtractor

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="PDF OCR Processor",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSã‚¹ã‚¿ã‚¤ãƒ«
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}
.success-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #d4edda;
    border: 1px solid #c3e6cb;
    margin: 1rem 0;
}
.error-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #f8d7da;
    border: 1px solid #f5c6cb;
    margin: 1rem 0;
}
.info-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #d1ecf1;
    border: 1px solid #bee5eb;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
st.markdown('<h1 class="main-header">ğŸ“„ PDF OCR Processor</h1>', unsafe_allow_html=True)
st.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.title("âš™ï¸ è¨­å®š")
enhancement_level = st.sidebar.selectbox(
    "OCRå‡¦ç†ãƒ¬ãƒ™ãƒ«",
    ["light", "standard", "aggressive"],
    index=1,
    help="å‡¦ç†ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã—ã¾ã™ãŒã€æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™"
)

st.sidebar.markdown("### å‡¦ç†ãƒ¬ãƒ™ãƒ«èª¬æ˜")
level_descriptions = {
    "light": "ğŸŸ¢ è»½é‡å‡¦ç† - é«˜é€Ÿã§ã™ãŒåŸºæœ¬çš„ãªOCR",
    "standard": "ğŸŸ¡ æ¨™æº–å‡¦ç† - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸç²¾åº¦ã¨é€Ÿåº¦",
    "aggressive": "ğŸ”´ é«˜ç²¾åº¦å‡¦ç† - æœ€é«˜ç²¾åº¦ã§ã™ãŒæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™"
}
st.sidebar.markdown(f"**{enhancement_level}**: {level_descriptions[enhancement_level]}")

# OpenAI APIè¨­å®š
st.sidebar.markdown("### OpenAI APIè¨­å®š")
api_key = st.sidebar.text_input(
    "OpenAI API Key",
    type="password",
    help="OCRçµæœã®æ ¡æ­£ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚è¨­å®šã—ãªãã¦ã‚‚åŸºæœ¬çš„ãªOCRã¯å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚"
)

if api_key:
    os.environ['OPENAI_API_KEY'] = api_key
    st.sidebar.success("API Keyè¨­å®šå®Œäº†")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
col1, col2 = st.columns([1, 1])

with col1:
    st.markdown("### ğŸ“ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['pdf'],
        accept_multiple_files=True,
        help="è¤‡æ•°ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸ")
        for file in uploaded_files:
            st.write(f"ğŸ“„ {file.name} ({file.size / 1024:.1f} KB)")

with col2:
    st.markdown("### ğŸ”§ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    
    show_progress = st.checkbox("å‡¦ç†é€²æ—ã‚’è¡¨ç¤º", value=True)
    show_word_list = st.checkbox("æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", value=True)
    show_passages = st.checkbox("è‹±èªæ–‡ç« ã‚’è¡¨ç¤º", value=True)
    include_stats = st.checkbox("è©³ç´°çµ±è¨ˆã‚’å«ã‚ã‚‹", value=True)

def process_files(uploaded_files, enhancement_level, show_progress, show_word_list, show_passages, include_stats):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    """
    try:
        # PDFTextExtractorã‚’åˆæœŸåŒ–
        extractor = PDFTextExtractor()
        
        results = []
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
        if show_progress:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        for i, uploaded_file in enumerate(uploaded_files):
            if show_progress:
                status_text.text(f"å‡¦ç†ä¸­: {uploaded_file.name}")
                progress_bar.progress((i) / len(uploaded_files))
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(uploaded_file.read())
                temp_file_path = temp_file.name
            
            try:
                # OCRå‡¦ç†å®Ÿè¡Œ
                result = extractor.process_pdf(temp_file_path, enhancement_level)
                
                # çµæœã‚’æ•´ç†
                processed_result = {
                    'file_info': {
                        'source_file': uploaded_file.name,
                        'file_size': uploaded_file.size,
                        'processed_pages': result.get('pages_processed', 0),
                        'ocr_confidence': result.get('processing_stats', {}).get('average_confidence', 0.0),
                        'processing_level': enhancement_level,
                        'processing_timestamp': datetime.now().isoformat(),
                        'error': result.get('error', None)
                    },
                    'extraction_results': {
                        'total_words': len(result.get('extracted_words', [])),
                        'unique_words': len(set(result.get('extracted_words', []))),
                        'english_passages_count': len(result.get('pure_english_text', [])),
                        'ocr_attempts': result.get('processing_stats', {}).get('total_ocr_attempts', 0),
                        'successful_extractions': result.get('processing_stats', {}).get('successful_extractions', 0)
                    },
                    'content': {
                        'english_passages': result.get('pure_english_text', []),
                        'extracted_words': sorted(list(set(result.get('extracted_words', []))))
                    }
                }
                
                results.append(processed_result)
                
            except Exception as e:
                st.error(f"âŒ {uploaded_file.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                results.append({
                    'file_info': {
                        'source_file': uploaded_file.name,
                        'error': str(e)
                    },
                    'extraction_results': {},
                    'content': {}
                })
            
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.unlink(temp_file_path)
        
        if show_progress:
            progress_bar.progress(1.0)
            status_text.text("å‡¦ç†å®Œäº†ï¼")
        
        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
        st.session_state.results = results
        
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        successful_files = [r for r in results if not r['file_info'].get('error')]
        if successful_files:
            st.markdown('<div class="success-box">âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼</div>', unsafe_allow_html=True)
        
    except Exception as e:
        st.markdown(f'<div class="error-box">âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}</div>', unsafe_allow_html=True)

def display_results(results, show_word_list, show_passages, include_stats):
    """
    å‡¦ç†çµæœã‚’è¡¨ç¤º
    """
    st.markdown("---")
    st.markdown("## ğŸ“Š å‡¦ç†çµæœ")
    
    # å…¨ä½“çµ±è¨ˆ
    total_words = sum(r['extraction_results'].get('total_words', 0) for r in results)
    total_unique_words = len(set(word for r in results for word in r['content'].get('extracted_words', [])))
    total_passages = sum(r['extraction_results'].get('english_passages_count', 0) for r in results)
    successful_files = [r for r in results if not r['file_info'].get('error')]
    
    # çµ±è¨ˆè¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°", f"{len(successful_files)}/{len(results)}")
    with col2:
        st.metric("æŠ½å‡ºå˜èªç·æ•°", total_words)
    with col3:
        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", total_unique_words)
    with col4:
        st.metric("è‹±èªæ–‡ç« æ•°", total_passages)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°çµæœ
    for i, result in enumerate(results):
        with st.expander(f"ğŸ“„ {result['file_info']['source_file']}", expanded=True):
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if result['file_info'].get('error'):
                st.error(f"ã‚¨ãƒ©ãƒ¼: {result['file_info']['error']}")
                continue
            
            # åŸºæœ¬æƒ…å ±
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æŠ½å‡ºå˜èªæ•°", result['extraction_results'].get('total_words', 0))
            with col2:
                st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", result['extraction_results'].get('unique_words', 0))
            with col3:
                st.metric("OCRä¿¡é ¼åº¦", f"{result['file_info'].get('ocr_confidence', 0):.3f}")
            
            # è©³ç´°çµ±è¨ˆ
            if include_stats:
                st.markdown("**å‡¦ç†è©³ç´°:**")
                stats_col1, stats_col2 = st.columns(2)
                with stats_col1:
                    st.write(f"- å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {result['file_info'].get('processed_pages', 0)}")
                    st.write(f"- å‡¦ç†ãƒ¬ãƒ™ãƒ«: {result['file_info'].get('processing_level', 'N/A')}")
                with stats_col2:
                    st.write(f"- OCRè©¦è¡Œå›æ•°: {result['extraction_results'].get('ocr_attempts', 0)}")
                    st.write(f"- æˆåŠŸæŠ½å‡ºæ•°: {result['extraction_results'].get('successful_extractions', 0)}")
            
            # è‹±èªæ–‡ç« è¡¨ç¤º
            if show_passages and result['content'].get('english_passages'):
                st.markdown("**ğŸ“ æŠ½å‡ºã•ã‚ŒãŸè‹±èªæ–‡ç« :**")
                for j, passage in enumerate(result['content']['english_passages'][:3]):  # æœ€åˆã®3æ–‡ç« 
                    with st.container():
                        st.markdown(f"**æ–‡ç«  {j+1}:**")
                        st.text_area(f"passage_{i}_{j}", passage, height=100, key=f"passage_{i}_{j}")
                
                if len(result['content']['english_passages']) > 3:
                    st.info(f"ä»–ã«{len(result['content']['english_passages']) - 3}æ–‡ç« ãŒã‚ã‚Šã¾ã™")
            
            # æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆè¡¨ç¤º
            if show_word_list and result['content'].get('extracted_words'):
                st.markdown("**ğŸ“š æŠ½å‡ºã•ã‚ŒãŸå˜èª:**")
                words = result['content']['extracted_words']
                
                # å˜èªã‚’è¡Œã”ã¨ã«è¡¨ç¤ºï¼ˆ20èªãšã¤ï¼‰
                for k in range(0, min(len(words), 100), 20):
                    word_group = words[k:k+20]
                    st.write(", ".join(word_group))
                
                if len(words) > 100:
                    st.info(f"ä»–ã«{len(words) - 100}èªãŒã‚ã‚Šã¾ã™")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.markdown("**ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:**")
            col1, col2 = st.columns(2)
            
            with col1:
                # JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                json_data = json.dumps(result, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“„ JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json_data,
                    file_name=f"{Path(result['file_info']['source_file']).stem}.json",
                    mime="application/json"
                )
            
            with col2:
                # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if result['content'].get('english_passages'):
                    text_data = "\n\n".join(result['content']['english_passages'])
                    st.download_button(
                        label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=text_data,
                        file_name=f"{Path(result['file_info']['source_file']).stem}.txt",
                        mime="text/plain"
                    )
    
    # å…¨ä½“çµæœã®ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if len(results) > 1:
        st.markdown("---")
        st.markdown("### ğŸ¯ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # å…¨çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§æä¾›
        zip_buffer = create_zip_download(results)
        st.download_button(
            label="ğŸ“¦ å…¨çµæœã‚’ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=zip_buffer,
            file_name=f"ocr_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip"
        )

def create_zip_download(results):
    """
    å‡¦ç†çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä½œæˆ
    """
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for result in results:
            if result['file_info'].get('error'):
                continue
            
            file_stem = Path(result['file_info']['source_file']).stem
            
            # JSONå½¢å¼ã§ä¿å­˜
            json_data = json.dumps(result, ensure_ascii=False, indent=2)
            zip_file.writestr(f"{file_stem}.json", json_data)
            
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜
            if result['content'].get('english_passages'):
                text_data = "\n\n".join(result['content']['english_passages'])
                zip_file.writestr(f"{file_stem}.txt", text_data)
            
            # å˜èªãƒªã‚¹ãƒˆã‚’ä¿å­˜
            if result['content'].get('extracted_words'):
                words_data = "\n".join(result['content']['extracted_words'])
                zip_file.writestr(f"{file_stem}_words.txt", words_data)
    
    zip_buffer.seek(0)
    return zip_buffer.getvalue()

# å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
if uploaded_files:
    if st.button("ğŸš€ OCRå‡¦ç†ã‚’é–‹å§‹", type="primary", use_container_width=True):
        process_files(uploaded_files, enhancement_level, show_progress, show_word_list, show_passages, include_stats)

# å‡¦ç†çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
if 'results' in st.session_state:
    display_results(st.session_state.results, show_word_list, show_passages, include_stats)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸ”§ PDF OCR Processor | é«˜ç²¾åº¦OCRå‡¦ç†ã¨LLMæ ¡æ­£ã«ã‚ˆã‚‹è‹±èªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º</p>
    <p>ğŸ“š Targetå˜èªå¸³ã¨ã®ç…§åˆåˆ†æã‚‚å¯èƒ½</p>
</div>
""", unsafe_allow_html=True)