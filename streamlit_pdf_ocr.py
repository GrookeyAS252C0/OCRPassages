import streamlit as st
import json
import tempfile
import os
from pathlib import Path
from datetime import datetime
import io
import zipfile
import time

# Streamlit Cloudç’°å¢ƒã§ã®NLTKãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
try:
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except:
    pass

from pdf_text_extractor import PDFTextExtractor

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="PDF OCR Processor",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSã‚¹ã‚¿ã‚¤ãƒ«
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}
.success-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #d4edda;
    border: 1px solid #c3e6cb;
    margin: 1rem 0;
}
.error-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #f8d7da;
    border: 1px solid #f5c6cb;
    margin: 1rem 0;
}
.info-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #d1ecf1;
    border: 1px solid #bee5eb;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
st.markdown('<h1 class="main-header">ğŸ“„ PDF OCR Processor</h1>', unsafe_allow_html=True)
st.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.title("âš™ï¸ è¨­å®š")
enhancement_level = st.sidebar.selectbox(
    "OCRå‡¦ç†ãƒ¬ãƒ™ãƒ«",
    ["light", "standard", "aggressive"],
    index=2,  # aggressive ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
    help="å‡¦ç†ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã—ã¾ã™ãŒã€æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™"
)

st.sidebar.markdown("### å‡¦ç†ãƒ¬ãƒ™ãƒ«èª¬æ˜")
level_descriptions = {
    "light": "ğŸŸ¢ è»½é‡å‡¦ç† - é«˜é€Ÿã§ã™ãŒåŸºæœ¬çš„ãªOCR",
    "standard": "ğŸŸ¡ æ¨™æº–å‡¦ç† - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸç²¾åº¦ã¨é€Ÿåº¦",
    "aggressive": "ğŸ”´ é«˜ç²¾åº¦å‡¦ç† - æœ€é«˜ç²¾åº¦ã§ã™ãŒæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™"
}
st.sidebar.markdown(f"**{enhancement_level}**: {level_descriptions[enhancement_level]}")

st.sidebar.markdown("### ğŸ¤– AIæ ¡æ­£è¨­å®š")
st.sidebar.info("""
**ãƒ¢ãƒ‡ãƒ«**: GPT-4o-mini  
**æ©Ÿèƒ½**: 
- OCRçµæœã®è‡ªå‹•æ ¡æ­£
- æ—¥æœ¬èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„é™¤å»
- ç´”ç²‹è‹±èªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ä¿®æ­£
""")

st.sidebar.markdown("### ğŸ“‹ å‡¦ç†ä»•æ§˜")
st.sidebar.markdown("""
- **è§£åƒåº¦**: 300 DPIå¤‰æ›
- **å‰å‡¦ç†**: 6ç¨®é¡ã®ç”»åƒå¼·åŒ–
- **OCR**: Tesseract + AIæ ¡æ­£
- **å‡ºåŠ›**: JSON/TXT/ZIPå½¢å¼
""")

# OpenAI APIè¨­å®šãƒã‚§ãƒƒã‚¯
if "OPENAI_API_KEY" in st.secrets:
    api_key = st.secrets["OPENAI_API_KEY"]
    os.environ['OPENAI_API_KEY'] = api_key
    
    # APIã‚­ãƒ¼æ¥ç¶šãƒ†ã‚¹ãƒˆ
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        # ç°¡å˜ãªAPIæ¥ç¶šãƒ†ã‚¹ãƒˆ
        client.models.list()
        st.sidebar.success("âœ… OpenAI APIæ¥ç¶šç¢ºèªæ¸ˆã¿")
    except Exception as e:
        st.sidebar.error("âŒ OpenAI APIæ¥ç¶šã‚¨ãƒ©ãƒ¼")
        st.error(f"""
        ğŸš¨ **OpenAI APIæ¥ç¶šã‚¨ãƒ©ãƒ¼**
        
        ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}
        
        **å¯¾å‡¦æ–¹æ³•:**
        1. Streamlit Cloudã®Secretsã§APIã‚­ãƒ¼ã‚’ç¢ºèª
        2. APIã‚­ãƒ¼ãŒæ­£ã—ã„å½¢å¼ã‹ç¢ºèª
        3. OpenAIã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ®‹é«˜ã‚’ç¢ºèª
        """)
        st.stop()
else:
    st.sidebar.error("âŒ OpenAI API Keyæœªè¨­å®š")
    st.error("""
    ğŸš¨ **OpenAI API KeyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“**
    
    **è¨­å®šæ–¹æ³•:**
    1. Streamlit Cloud â†’ Settings â†’ Secrets
    2. ä»¥ä¸‹ã‚’è¿½åŠ :
    ```
    OPENAI_API_KEY = "your-api-key-here"
    ```
    3. ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•
    
    è©³ç´°ã¯ `SECRETS_SETUP.md` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
    """)
    st.stop()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
col1, col2 = st.columns([1, 1])

with col1:
    st.markdown("### ğŸ“ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['pdf'],
        accept_multiple_files=True,
        help="è¤‡æ•°ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸ")
        for file in uploaded_files:
            st.write(f"ğŸ“„ {file.name} ({file.size / 1024:.1f} KB)")

with col2:
    st.markdown("### ğŸ”§ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    
    show_progress = st.checkbox("å‡¦ç†é€²æ—ã‚’è¡¨ç¤º", value=True)
    show_word_list = st.checkbox("æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", value=True)
    show_passages = st.checkbox("è‹±èªæ–‡ç« ã‚’è¡¨ç¤º", value=True)
    include_stats = st.checkbox("è©³ç´°çµ±è¨ˆã‚’å«ã‚ã‚‹", value=True)

def process_files(uploaded_files, enhancement_level, show_progress, show_word_list, show_passages, include_stats):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
    Args:
        uploaded_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        enhancement_level: OCRå‡¦ç†ãƒ¬ãƒ™ãƒ«
        show_progress: é€²æ—è¡¨ç¤ºãƒ•ãƒ©ã‚°
        show_word_list: å˜èªãƒªã‚¹ãƒˆè¡¨ç¤ºãƒ•ãƒ©ã‚°ï¼ˆçµæœè¡¨ç¤ºã§ä½¿ç”¨ï¼‰
        show_passages: è‹±èªæ–‡ç« è¡¨ç¤ºãƒ•ãƒ©ã‚°ï¼ˆçµæœè¡¨ç¤ºã§ä½¿ç”¨ï¼‰
        include_stats: è©³ç´°çµ±è¨ˆè¡¨ç¤ºãƒ•ãƒ©ã‚°ï¼ˆçµæœè¡¨ç¤ºã§ä½¿ç”¨ï¼‰
    """
    # è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜ï¼ˆçµæœè¡¨ç¤ºæ™‚ã«ä½¿ç”¨ï¼‰
    st.session_state.display_options = {
        'show_word_list': show_word_list,
        'show_passages': show_passages,
        'include_stats': include_stats
    }
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
    st.write("ğŸ” **ãƒ‡ãƒãƒƒã‚°æƒ…å ±**")
    debug_container = st.empty()
    error_container = st.empty()
    
    def log_debug(message):
        debug_container.text(f"[DEBUG] {message}")
    
    def log_error(message, error=None):
        error_msg = f"[ERROR] {message}"
        if error:
            error_msg += f" - {str(error)}"
        error_container.error(error_msg)
        print(error_msg)  # ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã«ã‚‚å‡ºåŠ›
    try:
        log_debug("PDFTextExtractorã‚’åˆæœŸåŒ–ä¸­...")
        # PDFTextExtractorã‚’åˆæœŸåŒ–
        extractor = PDFTextExtractor()
        log_debug("PDFTextExtractoråˆæœŸåŒ–å®Œäº†")
        
        results = []
        log_debug(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(uploaded_files)}")
        
        # å…¨ä½“ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        if show_progress:
            st.markdown("### ğŸ“Š å‡¦ç†é€²æ—")
            overall_progress = st.progress(0)
            overall_status = st.empty()
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°é€²æ—ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠ
            progress_container = st.container()
        
        for i, uploaded_file in enumerate(uploaded_files):
            log_debug(f"ãƒ•ã‚¡ã‚¤ãƒ« {i+1}/{len(uploaded_files)} å‡¦ç†é–‹å§‹: {uploaded_file.name}")
            
            if show_progress:
                # å…¨ä½“é€²æ—æ›´æ–°
                overall_progress.progress(i / len(uploaded_files))
                overall_status.text(f"å‡¦ç†ä¸­: {i+1}/{len(uploaded_files)} - {uploaded_file.name}")
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«é€²æ—è¡¨ç¤º
                with progress_container:
                    file_expander = st.expander(f"ğŸ“„ {uploaded_file.name} - å‡¦ç†ä¸­...", expanded=True)
                    with file_expander:
                        file_progress = st.progress(0)
                        file_status = st.empty()
                        step_status = st.empty()
                        
                        # ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
                        file_status.text("ğŸ”„ PDFãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
                        file_progress.progress(0.1)
            
            try:
                log_debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {uploaded_file.name}")
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                    file_data = uploaded_file.read()
                    log_debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(file_data)} bytes")
                    temp_file.write(file_data)
                    temp_file_path = temp_file.name
                log_debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {temp_file_path}")
                
                if show_progress:
                    file_status.text("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")
                    file_progress.progress(0.2)
            
                if show_progress:
                    file_status.text("ğŸ” OCRå‡¦ç†é–‹å§‹...")
                    file_progress.progress(0.3)
                    step_status.text(f"å‡¦ç†ãƒ¬ãƒ™ãƒ«: {enhancement_level}")
                
                log_debug(f"OCRå‡¦ç†é–‹å§‹: {enhancement_level}")
                # OCRå‡¦ç†å®Ÿè¡Œ
                result = extractor.process_pdf(temp_file_path, enhancement_level)
                log_debug(f"OCRå‡¦ç†å®Œäº†: {uploaded_file.name}")
                
                if show_progress:
                    file_status.text("ğŸ¤– AIæ ¡æ­£å‡¦ç†ä¸­...")
                    file_progress.progress(0.7)
                
                # çµæœã‚’æ•´ç†
                processed_result = {
                    'file_info': {
                        'source_file': uploaded_file.name,
                        'file_size': uploaded_file.size,
                        'processed_pages': result.get('pages_processed', 0),
                        'ocr_confidence': result.get('processing_stats', {}).get('average_confidence', 0.0),
                        'processing_level': enhancement_level,
                        'processing_timestamp': datetime.now().isoformat(),
                        'error': result.get('error', None)
                    },
                    'extraction_results': {
                        'total_words': len(result.get('extracted_words', [])),
                        'unique_words': len(set(result.get('extracted_words', []))),
                        'english_passages_count': len(result.get('pure_english_text', [])),
                        'ocr_attempts': result.get('processing_stats', {}).get('total_ocr_attempts', 0),
                        'successful_extractions': result.get('processing_stats', {}).get('successful_extractions', 0)
                    },
                    'content': {
                        'english_passages': result.get('pure_english_text', []),
                        'extracted_words': sorted(list(set(result.get('extracted_words', []))))
                    }
                }
                
                results.append(processed_result)
                
                if show_progress:
                    file_status.text("âœ… å‡¦ç†å®Œäº†!")
                    file_progress.progress(1.0)
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¡¨ç¤º
                    token_usage = result.get('token_usage', {})
                    input_tokens = token_usage.get('total_input_tokens', 0)
                    output_tokens = token_usage.get('total_output_tokens', 0)
                    cost_usd = token_usage.get('total_cost_usd', 0.0)
                    
                    step_status.text(f"æŠ½å‡ºèªæ•°: {processed_result['extraction_results']['total_words']}, "
                                   f"ä¿¡é ¼åº¦: {processed_result['file_info']['ocr_confidence']:.3f}")
                    
                    if input_tokens > 0 or output_tokens > 0:
                        step_status.text(f"ğŸ’° ãƒˆãƒ¼ã‚¯ãƒ³: {input_tokens + output_tokens:,} (${cost_usd:.4f})")
                    
                    # expanderã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°
                    file_expander.empty()
                    with progress_container:
                        completed_expander = st.expander(
                            f"âœ… {uploaded_file.name} - å®Œäº† "
                            f"({processed_result['extraction_results']['total_words']}èªæŠ½å‡º)", 
                            expanded=False
                        )
                        with completed_expander:
                            st.success(f"ğŸ“Š å‡¦ç†çµæœ: {processed_result['extraction_results']['total_words']}èª, "
                                     f"ä¿¡é ¼åº¦: {processed_result['file_info']['ocr_confidence']:.3f}")
                
            except Exception as e:
                error_type = type(e).__name__
                error_message = str(e)
                log_error(f"{uploaded_file.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error_type}", e)
                
                # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—
                import traceback
                error_traceback = traceback.format_exc()
                log_debug(f"ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{error_traceback}")
                
                st.error(f"âŒ {uploaded_file.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
                
                if show_progress:
                    file_status.text("âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ")
                    file_progress.progress(1.0)
                    step_status.text(f"ã‚¨ãƒ©ãƒ¼: {error_type}")
                    
                    # expanderã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°
                    file_expander.empty()
                    with progress_container:
                        error_expander = st.expander(f"âŒ {uploaded_file.name} - ã‚¨ãƒ©ãƒ¼", expanded=True)
                        with error_expander:
                            st.error(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}")
                            st.code(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}")
                
                results.append({
                    'file_info': {
                        'source_file': uploaded_file.name,
                        'error': error_message,
                        'error_type': error_type
                    },
                    'extraction_results': {},
                    'content': {}
                })
            
            except Exception as inner_e:
                log_error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} ã®å†…éƒ¨å‡¦ç†ã‚¨ãƒ©ãƒ¼", inner_e)
                # å†…éƒ¨ã‚¨ãƒ©ãƒ¼ã‚‚resultsã«è¿½åŠ 
                results.append({
                    'file_info': {
                        'source_file': uploaded_file.name,
                        'error': f"å†…éƒ¨å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(inner_e)}",
                        'error_type': type(inner_e).__name__
                    },
                    'extraction_results': {},
                    'content': {}
                })
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                try:
                    if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
                        log_debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_file_path}")
                except Exception as cleanup_error:
                    log_error(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {temp_file_path}", cleanup_error)
        
        if show_progress:
            overall_progress.progress(1.0)
            overall_status.text("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
            # å®Œäº†ã‚µãƒãƒªãƒ¼
            st.markdown("### ğŸ“ˆ å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
            successful_count = len([r for r in results if not r['file_info'].get('error')])
            total_words = sum(r['extraction_results'].get('total_words', 0) for r in results)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡åˆè¨ˆè¨ˆç®—
            total_input_tokens = sum(r.get('token_usage', {}).get('total_input_tokens', 0) for r in results)
            total_output_tokens = sum(r.get('token_usage', {}).get('total_output_tokens', 0) for r in results)
            total_cost = sum(r.get('token_usage', {}).get('total_cost_usd', 0.0) for r in results)
            total_api_calls = sum(r.get('token_usage', {}).get('api_calls', 0) for r in results)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«æ•°", f"{successful_count}/{len(results)}")
            with col2:
                st.metric("ç·æŠ½å‡ºèªæ•°", total_words)
            with col3:
                avg_confidence = sum(r['file_info'].get('ocr_confidence', 0) 
                                   for r in results if r['file_info'].get('ocr_confidence', 0) > 0)
                avg_confidence = avg_confidence / max(successful_count, 1)
                st.metric("å¹³å‡ä¿¡é ¼åº¦", f"{avg_confidence:.3f}")
            with col4:
                st.metric("APIæ–™é‡‘", f"${total_cost:.4f}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨è©³ç´°
            if total_input_tokens > 0 or total_output_tokens > 0:
                st.markdown("### ğŸ’° OpenAI APIä½¿ç”¨é‡è©³ç´°")
                token_col1, token_col2, token_col3 = st.columns(3)
                with token_col1:
                    st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{total_input_tokens:,}")
                with token_col2:
                    st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{total_output_tokens:,}")
                with token_col3:
                    st.metric("APIå‘¼ã³å‡ºã—å›æ•°", total_api_calls)
        
        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
        st.session_state.results = results
        
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        successful_files = [r for r in results if not r['file_info'].get('error')]
        failed_files = [r for r in results if r['file_info'].get('error')]
        
        log_debug(f"å‡¦ç†å®Œäº†: æˆåŠŸ {len(successful_files)}, å¤±æ•— {len(failed_files)}")
        
        if successful_files:
            st.markdown('<div class="success-box">âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼</div>', unsafe_allow_html=True)
        
        if failed_files:
            st.warning(f"âš ï¸ {len(failed_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
    except Exception as e:
        import traceback
        main_error_traceback = traceback.format_exc()
        log_error("ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ", e)
        log_debug(f"ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{main_error_traceback}")
        
        st.markdown(f'<div class="error-box">âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}</div>', unsafe_allow_html=True)
        st.code(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
        
        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’å±•é–‹å¯èƒ½ãªå½¢ã§è¡¨ç¤º
        with st.expander("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°æƒ…å ±", expanded=False):
            st.code(main_error_traceback)

def display_results(results, show_word_list, show_passages, include_stats):
    """
    å‡¦ç†çµæœã‚’è¡¨ç¤º
    """
    st.markdown("---")
    st.markdown("## ğŸ“Š å‡¦ç†çµæœ")
    
    # å…¨ä½“çµ±è¨ˆ
    total_words = sum(r['extraction_results'].get('total_words', 0) for r in results)
    total_unique_words = len(set(word for r in results for word in r['content'].get('extracted_words', [])))
    total_passages = sum(r['extraction_results'].get('english_passages_count', 0) for r in results)
    successful_files = [r for r in results if not r['file_info'].get('error')]
    
    # çµ±è¨ˆè¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°", f"{len(successful_files)}/{len(results)}")
    with col2:
        st.metric("æŠ½å‡ºå˜èªç·æ•°", total_words)
    with col3:
        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", total_unique_words)
    with col4:
        st.metric("è‹±èªæ–‡ç« æ•°", total_passages)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°çµæœ
    for i, result in enumerate(results):
        with st.expander(f"ğŸ“„ {result['file_info']['source_file']}", expanded=True):
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if result['file_info'].get('error'):
                st.error(f"ã‚¨ãƒ©ãƒ¼: {result['file_info']['error']}")
                continue
            
            # åŸºæœ¬æƒ…å ±
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æŠ½å‡ºå˜èªæ•°", result['extraction_results'].get('total_words', 0))
            with col2:
                st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", result['extraction_results'].get('unique_words', 0))
            with col3:
                st.metric("OCRä¿¡é ¼åº¦", f"{result['file_info'].get('ocr_confidence', 0):.3f}")
            with col4:
                token_usage = result.get('token_usage', {})
                cost = token_usage.get('total_cost_usd', 0.0)
                st.metric("APIæ–™é‡‘", f"${cost:.4f}" if cost > 0 else "ãªã—")
            
            # è©³ç´°çµ±è¨ˆ
            if include_stats:
                st.markdown("**å‡¦ç†è©³ç´°:**")
                stats_col1, stats_col2 = st.columns(2)
                with stats_col1:
                    st.write(f"- å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {result['file_info'].get('processed_pages', 0)}")
                    st.write(f"- å‡¦ç†ãƒ¬ãƒ™ãƒ«: {result['file_info'].get('processing_level', 'N/A')}")
                    # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±
                    token_usage = result.get('token_usage', {})
                    if token_usage.get('total_input_tokens', 0) > 0:
                        st.write(f"- å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {token_usage.get('total_input_tokens', 0):,}")
                        st.write(f"- å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {token_usage.get('total_output_tokens', 0):,}")
                with stats_col2:
                    st.write(f"- OCRè©¦è¡Œå›æ•°: {result['extraction_results'].get('ocr_attempts', 0)}")
                    st.write(f"- æˆåŠŸæŠ½å‡ºæ•°: {result['extraction_results'].get('successful_extractions', 0)}")
                    # APIæƒ…å ±
                    if token_usage.get('api_calls', 0) > 0:
                        st.write(f"- APIå‘¼ã³å‡ºã—: {token_usage.get('api_calls', 0)}å›")
                        st.write(f"- æ–™é‡‘: ${token_usage.get('total_cost_usd', 0.0):.4f}")
            
            # è‹±èªæ–‡ç« è¡¨ç¤º
            if show_passages and result['content'].get('english_passages'):
                st.markdown("**ğŸ“ æŠ½å‡ºã•ã‚ŒãŸè‹±èªæ–‡ç« :**")
                for j, passage in enumerate(result['content']['english_passages'][:3]):  # æœ€åˆã®3æ–‡ç« 
                    with st.container():
                        st.markdown(f"**æ–‡ç«  {j+1}:**")
                        st.text_area(f"passage_{i}_{j}", passage, height=100, key=f"passage_{i}_{j}")
                
                if len(result['content']['english_passages']) > 3:
                    st.info(f"ä»–ã«{len(result['content']['english_passages']) - 3}æ–‡ç« ãŒã‚ã‚Šã¾ã™")
            
            # æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆè¡¨ç¤º
            if show_word_list and result['content'].get('extracted_words'):
                st.markdown("**ğŸ“š æŠ½å‡ºã•ã‚ŒãŸå˜èª:**")
                words = result['content']['extracted_words']
                
                # å˜èªã‚’è¡Œã”ã¨ã«è¡¨ç¤ºï¼ˆ20èªãšã¤ï¼‰
                for k in range(0, min(len(words), 100), 20):
                    word_group = words[k:k+20]
                    st.write(", ".join(word_group))
                
                if len(words) > 100:
                    st.info(f"ä»–ã«{len(words) - 100}èªãŒã‚ã‚Šã¾ã™")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.markdown("**ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:**")
            col1, col2 = st.columns(2)
            
            with col1:
                # JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                json_data = json.dumps(result, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“„ JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json_data,
                    file_name=f"{Path(result['file_info']['source_file']).stem}.json",
                    mime="application/json"
                )
            
            with col2:
                # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if result['content'].get('english_passages'):
                    text_data = "\n\n".join(result['content']['english_passages'])
                    st.download_button(
                        label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=text_data,
                        file_name=f"{Path(result['file_info']['source_file']).stem}.txt",
                        mime="text/plain"
                    )
    
    # å…¨ä½“çµæœã®ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if len(results) > 1:
        st.markdown("---")
        st.markdown("### ğŸ¯ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # å…¨çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§æä¾›
        zip_buffer = create_zip_download(results)
        st.download_button(
            label="ğŸ“¦ å…¨çµæœã‚’ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=zip_buffer,
            file_name=f"ocr_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip"
        )

def create_zip_download(results):
    """
    å‡¦ç†çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä½œæˆ
    """
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for result in results:
            if result['file_info'].get('error'):
                continue
            
            file_stem = Path(result['file_info']['source_file']).stem
            
            # JSONå½¢å¼ã§ä¿å­˜
            json_data = json.dumps(result, ensure_ascii=False, indent=2)
            zip_file.writestr(f"{file_stem}.json", json_data)
            
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜
            if result['content'].get('english_passages'):
                text_data = "\n\n".join(result['content']['english_passages'])
                zip_file.writestr(f"{file_stem}.txt", text_data)
            
            # å˜èªãƒªã‚¹ãƒˆã‚’ä¿å­˜
            if result['content'].get('extracted_words'):
                words_data = "\n".join(result['content']['extracted_words'])
                zip_file.writestr(f"{file_stem}_words.txt", words_data)
    
    zip_buffer.seek(0)
    return zip_buffer.getvalue()

# å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
if uploaded_files:
    if st.button("ğŸš€ OCRå‡¦ç†ã‚’é–‹å§‹", type="primary", use_container_width=True):
        process_files(uploaded_files, enhancement_level, show_progress, show_word_list, show_passages, include_stats)

# å‡¦ç†çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
if 'results' in st.session_state:
    # è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    display_options = st.session_state.get('display_options', {
        'show_word_list': True,
        'show_passages': True,
        'include_stats': True
    })
    display_results(
        st.session_state.results, 
        display_options['show_word_list'],
        display_options['show_passages'],
        display_options['include_stats']
    )

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸ”§ PDF OCR Processor | é«˜ç²¾åº¦OCRå‡¦ç†ã¨LLMæ ¡æ­£ã«ã‚ˆã‚‹è‹±èªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º</p>
    <p>ğŸ“š Targetå˜èªå¸³ã¨ã®ç…§åˆåˆ†æã‚‚å¯èƒ½</p>
</div>
""", unsafe_allow_html=True)